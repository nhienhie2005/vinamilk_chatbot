import torch

# Patch to√†n b·ªô torch.nn.Module.to ƒë·ªÉ tr√°nh l·ªói CUDA tr√™n Streamlit Cloud
def patched_to(self, device):
    return self  # B·ªè qua .to(), gi·ªØ nguy√™n ·ªü CPU

torch.nn.Module.to = patched_to

import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import os
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')




import random
import time
import base64
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from langchain_community.llms import CTransformers
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import pandas as pd
import io
import contextlib
import sys

# T·∫£i t√†i nguy√™n NLTK
nltk.download('punkt')
stemmer = PorterStemmer()

import os

# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
os.makedirs("models", exist_ok=True)

import gdown

# T·∫£i vinallama-7b-chat_q5_0.gguf
if not os.path.exists("models/vinallama-7b-chat_q5_0.gguf"):
    print("Downloading vinallama-7b-chat_q5_0.gguf from Google Drive...")
    gdown.download(
        id="1y6bUTofGFcQMtMjpLEqbe_R1DyAFyTi_",  
        output="models/vinallama-7b-chat_q5_0.gguf",
        quiet=False
    )

# T·∫£i all-MiniLM-L6-v2-f16.gguf
if not os.path.exists("models/all-MiniLM-L6-v2-f16.gguf"):
    print("Downloading all-MiniLM-L6-v2-f16.gguf from Google Drive...")
    gdown.download(
        id="111OhdcksuuAIPctvwntSLKUTs9Iy35p3",  
        output="models/all-MiniLM-L6-v2-f16.gguf",
        quiet=False
    )



from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Khai b√°o bi·∫øn
pdf_data_path = "data"
vector_db_path = "vectorstores/db_faiss"

def create_db_from_files():
    # Khai bao loader de quet toan bo thu muc data
    loader = DirectoryLoader(pdf_data_path, glob="*.pdf", loader_cls = PyPDFLoader)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)
    chunks = text_splitter.split_documents(documents)

    # Embedding
    embedding_model = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"device": "cpu"}
)

    db = FAISS.from_documents(chunks, embedding_model)
    db.save_local(vector_db_path)
    return db

create_db_from_files()

# ƒê·ªãnh nghƒ©a d·ªØ li·ªáu intents
intents = {
    "intents": [
        {"tag": "chao_hoi", "patterns": ["Ch√†o", "Hi", "Hello", "Xin ch√†o", "Ch√†o bu·ªïi s√°ng", "Ch√†o bu·ªïi chi·ªÅu", "Ch√†o bu·ªïi t·ªëi", "ch√†o b·∫°n"], 
         "responses": ["Ch√†o b·∫°n! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ Vinamilk?", "Xin ch√†o! B·∫°n mu·ªën t√¨m hi·ªÉu ƒëi·ªÅu g√¨ v·ªÅ Vinamilk?", "Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi Vinamilk!", "R·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ Vinamilk nh√©!"]},
        {"tag": "tam_biet", "patterns": ["T·∫°m bi·ªát", "Ch√†o t·∫°m bi·ªát", "Bye", "C·∫£m ∆°n b·∫°n nha!", "c·∫£m ∆°n"], 
         "responses": ["T·∫°m bi·ªát b·∫°n! H·∫πn g·∫∑p l·∫°i", "Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh!", "H·∫πn g·∫∑p l·∫°i b·∫°n trong l·∫ßn tr√≤ chuy·ªán ti·∫øp theo!", "C·∫£m ∆°n b·∫°n ƒë√£ gh√© thƒÉm Vinamilk!", "iu iuüíï", "H·∫πn g·∫∑p l·∫°i b·∫°n nh√©! Ch√∫c b·∫°n m·ªôt ng√†y tuy·ªát v·ªùi c√πng Vinamilkü•õ‚ù§Ô∏è"]}
    ]
}

def tokenize(sentence):
    return word_tokenize(sentence)

def stem(word):
    return stemmer.stem(word.lower())

def bag_of_words(tokenized_sentence, all_words):
    sentence_words = [stem(w) for w in tokenized_sentence]
    bag = np.zeros(len(all_words), dtype=np.float32)
    for idx, w in enumerate(all_words):
        if w in sentence_words:
            bag[idx] = 1.0
    return bag

all_words = []
tags = []
xy = []
for intent in intents["intents"]:
    tag = intent["tag"]
    tags.append(tag)
    for pattern in intent["patterns"]:
        w = tokenize(pattern)
        all_words.extend(w)
        xy.append((w, tag))
ignore_words = ['?', '.', '!', ',']
all_words = sorted(set([stem(w) for w in all_words if w not in ignore_words]))
tags = sorted(set(tags))
X_train = []
y_train = []
for (pattern_sentence, tag) in xy:
    bag = bag_of_words(pattern_sentence, all_words)
    X_train.append(bag)
    label = tags.index(tag)
    y_train.append(label)
X_train = np.array(X_train)
y_train = np.array(y_train)

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNet, self).__init__()
        self.l1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.l2 = nn.Linear(hidden_size, output_size)
    def forward(self, x):
        out = self.relu(self.l1(x))
        out = self.l2(out)
        return out

input_size = len(all_words)
hidden_size = 8
output_size = len(tags)
model = NeuralNet(input_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
inputs = torch.from_numpy(X_train).float()
targets = torch.from_numpy(y_train).long()
for epoch in range(500):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
model.eval()

# ·∫¢nh n·ªÅn base64
def get_base64_of_bin_file(bin_file):
    try:
        with open(bin_file, 'rb') as f:
            data = f.read()
        return base64.b64encode(data).decode()
    except FileNotFoundError:
        st.warning("Kh√¥ng t√¨m th·∫•y file h√¨nh n·ªÅn.")
        return None
img_path = "veny/Vinamilk.png"
img_base64 = get_base64_of_bin_file(img_path)

# Giao di·ªán
st.markdown(
    f"""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;600&display=swap');
    html, body, .stApp {{ font-family: 'Roboto', sans-serif !important; }}
    .stApp {{ background: {'#f0f2f6' if not img_base64 else f'url("data:image/png;base64,{img_base64}")'}; background-size: cover; background-position: center; background-attachment: fixed; position: relative; min-height: 100vh; }}
    .stApp::before {{ content: ''; position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.4); z-index: -1; }}
    h1 {{ text-align: center; color: white !important; margin-bottom: 30px; font-size: 2.5em; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); }}
    .bot-msg {{ background-color: #0071CE !important; color: #ffffff !important; padding: 12px 18px !important; border-radius: 15px !important; margin-bottom: 25px !important; display: inline-block !important; font-size: 1.1em !important; border: 1px solid #ffffff !important; background: linear-gradient(145deg, #0071CE, #005bb5) !important; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3) !important; }}
    .user-msg {{ background-color: #e0f0ff !important; color: #000000 !important; padding: 12px 18px !important; border-radius: 15px !important; margin-bottom: 25px !important; display: inline-block !important; font-size: 1.1em !important; border: 1px solid #b0d4f1 !important; background: linear-gradient(145deg, #e0f0ff, #c3e0ff) !important; box-shadow: none !important; float: right !important; text-align: right !important; margin-left: auto !important; margin-right: 0 !important; max-width: 80% !important; }}
    .stChatMessage {{ background-color: transparent !important; box-shadow: none !important; padding: 0 !important; margin-bottom: 0 !important; }}
    .stTextInput > div > div > input {{ font-size: 1.1em !important; padding: 12px !important; border: 1px solid #333333 !important; border-radius: 15px !important; background-color: #ffffff !important; }}
    .stChatInputContainer {{ box-shadow: none !important; background: none !important; }}
    </style>
    """,
    unsafe_allow_html=True
)


@st.cache_resource
def load_vector_db():
    embedding_model = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"device": "cpu"}
    )
    return FAISS.load_local("vectorstores/db_faiss", embedding_model, allow_dangerous_deserialization=True)

@st.cache_resource
def load_large_llm():
    return CTransformers(model="models/vinallama-7b-chat_q5_0.gguf", model_type="llama", max_new_tokens=1024, temperature=0.01)

db = load_vector_db()
llm = load_large_llm()
template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë th√¥ng tin n·ªôi b·ªô c·ªßa c√¥ng ty:

{context}

D·ª±a v√†o th√¥ng tin tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau:
C√¢u h·ªèi: {question}
Tr·∫£ l·ªùi:"""
prompt = PromptTemplate(template=template, input_variables=["context", "question"])
llm_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=db.as_retriever(search_kwargs={"k": 2}), return_source_documents=False, chain_type_kwargs={"prompt": prompt})

df_nv = pd.read_csv("data/tra_cuu_nhan_vien.csv", encoding="utf-8-sig")

def tra_cuu_nhan_vien(question):
    question_lower = question.lower()
    for _, row in df_nv.iterrows():
        try:
            ma_nv = int(row["M√£ nh√¢n vi√™n"]) if not pd.isna(row["M√£ nh√¢n vi√™n"]) else None
            ten_nv = row["H·ªç v√† t√™n"].lower() if not pd.isna(row["H·ªç v√† t√™n"]) else ""
        except:
            continue
        if (ma_nv is not None and str(ma_nv) in question_lower) or (ten_nv in question_lower):
            return row
    return None

def trich_thong_tin_yeu_cau(question, row):
    question = question.lower()
    response_parts = []
    if "ph√≤ng ban" in question and not pd.isna(row.get("Ph√≤ng ban")):
        response_parts.append(f"Ph√≤ng ban c·ªßa {row['H·ªç v√† t√™n']} l√†: {row['Ph√≤ng ban']}")
    if "ch·ª©c v·ª•" in question and not pd.isna(row.get("Ch·ª©c v·ª•")):
        response_parts.append(f"Ch·ª©c v·ª• c·ªßa {row['H·ªç v√† t√™n']} l√†: {row['Ch·ª©c v·ª•']}")
    if "m√£ nh√¢n vi√™n" in question and not pd.isna(row.get("M√£ nh√¢n vi√™n")):
        response_parts.append(f"M√£ nh√¢n vi√™n c·ªßa {row['H·ªç v√† t√™n']} l√†: {int(row['M√£ nh√¢n vi√™n'])}")
    if "ng√†y v√†o" in question and not pd.isna(row.get("Ng√†y v√†o c√¥ng ty")):
        response_parts.append(f"Ng√†y v√†o c√¥ng ty c·ªßa {row['H·ªç v√† t√™n']} l√†: {row['Ng√†y v√†o c√¥ng ty']}")
    if "ngh·ªâ ph√©p" in question and not pd.isna(row.get("S·ªë ng√†y ngh·ªâ ph√©p c√≤n l·∫°i")):
        response_parts.append(f"{row['H·ªç v√† t√™n']} c√≤n l·∫°i {int(row['S·ªë ng√†y ngh·ªâ ph√©p c√≤n l·∫°i'])} ng√†y ngh·ªâ ph√©p")
    if not response_parts:
        return f'''Th√¥ng tin c·ªßa {row["H·ªç v√† t√™n"]}:\n
- M√£ nh√¢n vi√™n: {int(row["M√£ nh√¢n vi√™n"])}\n
- Ph√≤ng ban: {row["Ph√≤ng ban"]}\n
- Ch·ª©c v·ª•: {row["Ch·ª©c v·ª•"]}\n
- Ng√†y v√†o c√¥ng ty: {row["Ng√†y v√†o c√¥ng ty"]}\n
- S·ªë ng√†y ngh·ªâ ph√©p c√≤n l·∫°i: {int(row["S·ªë ng√†y ngh·ªâ ph√©p c√≤n l·∫°i"])}'''
    return "\n".join(response_parts)

def chatbot(question):
    # Ki·ªÉm tra intents tr∆∞·ªõc
    tokenized_question = tokenize(question)
    bag = bag_of_words(tokenized_question, all_words)
    bag_tensor = torch.from_numpy(bag).float().to('cpu')
    
    with torch.no_grad():
        model.eval()
        model.to('cpu')
        output = model(bag_tensor)
        probabilities = torch.softmax(output, dim=0)
        predicted_index = torch.argmax(probabilities).item()
        confidence = probabilities[predicted_index].item()
    
    # tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi t·ª´ intents
    if confidence > 0.75:
        tag = tags[predicted_index]
        for intent in intents["intents"]:
            if intent["tag"] == tag:
                return random.choice(intent["responses"])
    row = tra_cuu_nhan_vien(question)
    if row is not None and not row.isnull().all():
        return trich_thong_tin_yeu_cau(question, row)
    if any(x in question.lower() for x in ["nh√¢n vi√™n", "ch·ª©c v·ª•", "ph√≤ng ban", "ngh·ªâ ph√©p", "m√£ nh√¢n vi√™n", "ng√†y v√†o"]):
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin b·∫°n c·∫ßn!"
    with contextlib.redirect_stdout(io.StringIO()):
        response = llm_chain.invoke({"query": question})
    return response["result"]

st.markdown('<h1>Vinamilk Chatbot üêÑ</h1>', unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "bot", "content": "Ch√†o b·∫°n! H√£y h·ªèi t√¥i v·ªÅ Vinamilk."}]
# Render tin nh·∫Øn ban ƒë·∫ßu ngay khi t·∫£i trang
for message in st.session_state.messages:
    if message["role"] == "bot":
        with st.chat_message("bot", avatar="veny/Logo Vinamilk.png"):
            st.markdown(f'<div class="bot-msg" style="margin-bottom:25px;">{message["content"]}</div>', unsafe_allow_html=True)
    else:
        with st.chat_message("user"):
            st.markdown(f'<div class="user-msg" style="margin-bottom:25px;">{message["content"]}</div>', unsafe_allow_html=True)

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(f'<div class="user-msg" style="margin-bottom:25px;"><span class="avatar"></span>{prompt}</div>', unsafe_allow_html=True)
    
    # Th√™m tin nh·∫Øn t·∫°m th·ªùi "ƒêang t√¨m ki·∫øm th√¥ng tin..."
    with st.chat_message("bot", avatar="veny/Logo Vinamilk.png"):
        st.markdown('<div class="bot-msg" style="margin-bottom:25px;">ƒêang t√¨m ki·∫øm th√¥ng tin...</div>', unsafe_allow_html=True)
        st.session_state.messages.append({"role": "bot", "content": "ƒêang t√¨m ki·∫øm th√¥ng tin..."})
    
    # Th·ª±c hi·ªán x·ª≠ l√Ω
    response = chatbot(prompt)

    # Render hi·ªáu ·ª©ng g√µ ch·ªØ
    with st.chat_message("bot", avatar="veny/Logo Vinamilk.png"):
        placeholder = st.empty()
        displayed_text = ""
        for char in response:
            displayed_text += char
            placeholder.markdown(f'<div class="bot-msg" style="margin-bottom:25px;">{displayed_text}</div>', unsafe_allow_html=True)
            time.sleep(0.02)  # ƒê·ªô tr·ªÖ 0.05 gi√¢y gi·ªØa c√°c k√Ω t·ª±
        placeholder.markdown(f'<div class="bot-msg" style="margin-bottom:25px;">{response}</div>', unsafe_allow_html=True)
    
    # hi·ªáu ·ª©ng g√µ 
    st.session_state.messages[-1] = {"role": "bot", "content": response}

    # Render l·∫°i to√†n b·ªô tin nh·∫Øn, bao g·ªìm tin nh·∫Øn v·ª´a c·∫≠p nh·∫≠t
    st.rerun()

  



